<!DOCTYPE HTML>
<!--
	TXT by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>NALOMA</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" rel="stylesheet">

		<!-- Google tag (gtag.js) -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-JCN5WMTYBF"></script>
		<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'G-JCN5WMTYBF');
		</script>
	</head>
	<body class="homepage is-preload">

		<div id="page-wrapper">

			<!-- Header -->
				<header id="header">
					<div class="logo container">
						<div>
							<span id="logo" style="display: none;">NALOMA∀</span><h1>NA</h1><p>tural</p> <h1>LO</h1><p>gic meets</p> <h1>MA</h1><p>chine learning</p> 
							<!-- <p>Natural Logic Meets Machine Learning</p> -->
						</div>
					</div>
				</header>

			<!-- Nav -->
				<nav id="nav">
					<ul>
						<li class="current"><a href="index.html">NALOMA∀</a></li>
						<li><a href="#program">Program</a></li>
						<li><a href="#keynotes">Keynotes</a></li>
						<li><a href="#call-for-papers">Call for papers</a></li>
						<li><a href="#important-dates">Important dates</a></li>
						<li><a href="#program-committee">Program committee</a></li>
						<!-- <li><a href="left-sidebar.html">left sidebar</a></li>
						<li><a href="right-sidebar.html">right sidebar</a></li>
						<li><a href="no-sidebar.html">no sidebar</a></li> -->
						<li><a href="#">Archives</a>
							<ul>
								<li><a href="https://aclanthology.org/venues/naloma/">ACL Anthology Proceedings</a></li>
								<li><a href="https://doi.org/10.1007/s10849-023-09408-y">JoLLI Special Issue</a></li>
								<li><a href="">Previous NALOMAs</a>
									<ul>
										<li><a href="https://sites.google.com/view/naloma4">NALOMA IV (2023)</a></li>
										<li><a href="https://sites.google.com/view/naloma22">NALOMA III (2022)</a></li>
										<li><a href="https://typo.uni-konstanz.de/naloma21/">NALOMA II (2021)</a></li>
										<li><a href="http://web.archive.org/web/20200923002008/https://typo.uni-konstanz.de/naloma20/">NALOMA I (2020)</a></li>
									</ul>
								</li>
							</ul>
						</li>	
					</ul>
				</nav>

			<!-- Banner -->
				<section id="banner">
					<div class="content">
						<h2>5th NALOMA is co-located with <a href="https://2025.esslli.eu/">ESSLLI</a> | 4-8 August 2025, Bochum (Germany)</h2>
						<!-- <a href="#main" class="button scrolly">Alright let's go</a> -->
					</div>
				</section>

			<!-- Main -->
				<section id="main">
					<div class="container">
						<div class="row gtr-200">
							<!-- <div class="col-12"> -->

								<!-- Highlight -->
									<!-- <section class="box highlight">
										<ul class="special">
											<li><a href="#" class="icon solid fa-search"><span class="label">Magnifier</span></a></li>
											<li><a href="#" class="icon solid fa-tablet-alt"><span class="label">Tablet</span></a></li>
											<li><a href="#" class="icon solid fa-flask"><span class="label">Flask</span></a></li>
											<li><a href="#" class="icon solid fa-cog"><span class="label">Cog?</span></a></li>
										</ul>
										<header>
											<h2>A random assortment of icons in circles</h2>
											<p>And some text that attempts to explain their significance</p>
										</header>
										<p>
											Phasellus quam turpis, feugiat sit amet ornare in, hendrerit in lectus. Praesent semper mod quis eget mi. Etiam eu<br />
											ante risus. Aliquam erat volutpat. Aliquam luctus et mattis lectus amet pulvinar. Nam nec turpis consequat.
										</p>
									</section> -->

							<!-- </div> -->
							<div class="col-12">

								<!-- Intro -->
									<section id="intro" class="box features">
										<!-- <h2 class="major"><span></span></h2> -->
											<div class="row">
													<div>
														There has been an ever-growing interest in tasks targeting Natural Language Understanding and Reasoning. Although deep learning models have achieved human-like performance in many such tasks, it has also been repeatedly shown that they lack the precision, generalization power, reasoning capabilities, and explainability found in more traditional, symbolic approaches. Thus, current research has started employing hybrid methods, combining the strengths of each tradition and mitigating its weaknesses. This workshop would like to promote this research direction and foster fruitful dialog between the two disciplines by bringing together researchers working on hybrid methods in any subfield of Natural Language Understanding and Reasoning.
													</div>
													<div>
														NALOMA began by focusing on bridging the gap between machine learning and natural logic but has since broadened to include works integrating symbolic methods and machine learning. Even so, combining deep learning with natural logic remains a highly promising direction. The remarkable capabilities of LLMs, which directly operate on natural language expressions, offer a distinct advantage to natural logic&mdash;a family of logics with formulas resembling natural language expressions.
													</div>  
													<div>The NALOMA workshop is endorsed by <a href="https://www.sigsem.org/">SIGSEM</a>.</div>
											</div>
									</section>

								<!-- Program -->

									<section id="program" class="box features">
										<h2 class="major"><span>Workshop Program</span></h2>
										<div class="workshop-program">
											<div class="workshop-program-calendar" id="calendar"></div>
									
											<div class="workshop-program-abstracts">
												<h2>Talk Abstracts</h2>
												<div class="workshop-program-abstract keynote-abstract" id="keynote1">
													<h3><a href="#program" title="Jump up">⤴</a> <strong>Keynote:</strong> Understanding Complex Situation Descriptions</h3>
													<p><strong>Aaron Steven White</strong></p>
													<p>We use natural language to convey information about situations: things that happen or stuff that is true. This ability is supported by systematic relationships between the way we conceptualize situations and the way we describe them. These systematic relationships in turn underwrite inferences that go beyond what one strictly says in describing a situation. The question that motivates this talk is how to design systems that correctly capture the inferences we draw about situations on the basis of their descriptions.
													<br>
													Classical approaches to this question–exemplified in their modern form by graph-based representations, such Uniform Meaning Representation–attempt to capture the situation conceptualization associated with a description using a symbolic situation ontology and to draw inferences on the basis of rules stated over that ontology. An increasingly popular alternative to such ontology-factored approaches are ontology-free approaches, which attempt to directly represent inferences about a situation as natural language strings associated with a situation description, thereby bypassing the problem of engineering a situation ontology entirely. 
													<br>
													I discuss the benefits and drawbacks of these two approaches and present case studies in synthesizing them that focus specifically on how best to capture inferences about complex situations–i.e. situations, like building a house, that themselves may be composed of subsituations, like laying the house’s foundations, framing the house, etc. I argue that we should ultimately strive for ontology-free representations but that the challenges inherent to reasoning about complex situations highlight the persistent benefits of situation ontologies in providing representational scaffolding for the construction and evaluation of such representations.</p>
												</div>
												<div class="workshop-program-abstract keynote-abstract" id="keynote2">
													<h3><a href="#program" title="Jump up">⤴</a> <strong>Keynote:</strong> How can large language model become more human?</h3>
													<p><strong>Mehrnoosh Sadrzadeh</strong> (Joint work with Daphne Wang, Miloš Stanojević, Wing-Yee Chow, and Richard Breheny)</p>
													<p>Psycholinguistic experiments reveal that efficiency of human language use is founded on predictions at both syntactic and lexical levels. Previous models of human prediction exploiting LLMs have used an information theoretic measure called surprisal, with success on naturalistic text in a wide variety of languages, but under-performance on challenging text such as garden path sentences. This paper introduces a novel framework that combines the lexical predictions of an LLM with the syntactic structures provided by a dependency parser. The framework gives rise to an Incompatibility Fraction. When tested on two garden path datasets, it correlated well with human reading times, distinguished between easy and hard garden path, and outperformed surprisal.</p>
												</div>
												<div class="workshop-program-abstract keynote-abstract" id="keynote3">
													<h3><a href="#program" title="Jump up">⤴</a> <strong>Keynote:</strong> Understanding the Logic of Generative AI through Logic</h3>
													<p><strong>Kyle Richardson</strong></p>
													<p>Symbolic logic has long served as the de-facto language for expressing complex knowledge throughout computer science, owing to its clean semantics. Symbolic approaches to reasoning that are driven by declarative knowledge, in sharp contrast to purely machine learning-based approaches, have the advantage of allowing us to reason transparently about the behavior and correctness of the resulting systems. In this talk, we focus on the broad question: Can the declarative approach be leveraged to better understand and formally specify algorithms for large language models (LLMs)? We focus on formalizing recent direct preference alignment (DPA) loss functions, such as DPO, that are currently at the forefront of LLM alignment. Specifically, we ask: Given an existing DPA loss, can we systematically derive a symbolic expression that characterizes its semantics? We outline the details of a novel formalism we developed for these purposes. We also discuss how this formal view of preference learning sheds new light on both the size and structure of the DPA loss landscape and makes it possible to derive new alignment algorithms from first principles. Our framework and approach aim not only to provide guidance for the AI alignment community, but also to open up new opportunities for researchers in formal semantics to engage more directly with the development and analysis of LLM algorithms.</p>
												</div>
												<div class="workshop-program-abstract" id="talk1">
													<h3><a href="#program" title="Jump up">⤴</a> <strong>Paper:</strong> Implementing a Logical Inference System for Japanese Comparatives</h3>
													<p><strong>Yosuke Mikami, Daiki Matsuoka, Hitomi Yanaka</strong></p>
													<p>Natural Language Inference (NLI) involving comparatives is challenging because it requires understanding quantities and comparative relations expressed by sentences. While some approaches leverage Large Language Models (LLMs), we focus on logic-based approaches grounded in compositional semantics, which are promising for robust handling of numerical and logical expressions. Previous studies along these lines have proposed logical inference systems for English comparatives. However, it has been pointed out that there are several morphological and semantic differences between Japanese and English comparatives. These differences make it difficult to apply such systems directly to Japanese comparatives. To address this gap, this study proposes ccg-jcomp, a logical inference system for Japanese comparatives based on compositional semantics. We evaluate the proposed system on a Japanese NLI dataset containing comparative expressions. We demonstrate the effectiveness of our system by comparing its accuracy with that of existing LLMs.</p>
												</div>
												<div class="workshop-program-abstract" id="talk2">
													<h3><a href="#program" title="Jump up">⤴</a> <strong>Paper:</strong> Unpacking Legal Reasoning in LLMs: Chain-of-Thought as a Key to Human-Machine Alignment in Essay-Based NLU Tasks</h3>
													<p><strong>Ying-Chu Yu, Sieh-Chuen Huang, Hsuan-Lei Shao</strong> </p>
													<p>This study evaluates how Large Language Models (LLMs) perform deep legal reasoning on Taiwanese Status Law questions and investigates how Chain-of-Thought (CoT) prompting affects interpretability, alignment, and generalization. Using a two-stage evaluation framework, we first decomposed six real legal essay questions into 68 sub-questions covering issue spotting, statutory application, and inheritance computation. In Stage Two, full-length answers were collected under baseline and CoT-prompted conditions. Four LLMs—ChatGPT-4o, Gemini, Grok3, and Copilot—were tested. Results show CoT prompting significantly improved accuracy for Gemini (from 83.2% to 94.5%, p < 0.05) and Grok3, with moderate but consistent gains for ChatGPT and Copilot. Human evaluation of full-length responses revealed CoT answers received notably higher scores in issue coverage and reasoning clarity, with ChatGPT and Gemini gaining +2.67 and +1.92 points respectively. Despite these gains, legal misclassifications persist, highlighting alignment gaps between surface-level fluency and expert legal reasoning. This work opens the black box of legal NLU by tracing LLM reasoning chains, quantifying performance shifts under structured prompting, and providing a diagnostic benchmark for complex, open-ended legal tasks beyond multiple-choice settings.</p>
												</div>
												<div class="workshop-program-abstract" id="talk3">
													<h3><a href="#program" title="Jump up">⤴</a> <strong>Paper:</strong> Dataset Creation for Visual Entailment using Generative AI</h3>
													<p><strong>Rob Reijtenbach, Suzan Verberne, Gijs Wijnholds</strong></p>
													<p>In this paper we present and validate a new synthetic dataset for training visual entailment models. Existing datasets for visual entailment are small and sparse compared to datasets for textual entailment. Manually creating datasets is labor-intensive. We base our synthetic dataset on the SNLI dataset for textual entailment. We take the premise text from SNLI as input prompts in a generative image model, Stable Diffusion, creating an image to replace each textual premise. We evaluate our dataset both intrinsically and extrinsically. For extrinsic evaluation, we evaluate the validity of the generated images by using them as training data for a visual entailment classifier based on CLIP feature vectors. We find that synthetic training data only leads to a slight drop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 when trained on real data. We also compare the quality of our generated training data to original training data on another dataset: SICK-VTE. Again, there is only a slight drop in F-score: from 0.400 to 0.384. These results indicate that in settings with data sparsity, synthetic data can be a promising solution for training visual entailment models.</p>
												</div>
												<div class="workshop-program-abstract" id="talk4">
													<h3><a href="#program" title="Jump up">⤴</a> <strong>Paper:</strong> In the Mood for Inference: Logic-Based Natural Language Inference with Large Language Models</h3>
													<p><strong>Bill Noble, Rasmus Blanck, Gijs Wijnholds</strong></p>
													<p>In this paper we explore challenging Natural Language Inference datasets with a logic-based approach and Large Language Models (LLMs), in order to assess the validity of this hybrid strategy. We report on an experiment which combines an LLM meta-prompting strategy, eliciting logical representations, and Prover9, a first-order logic theorem prover. In addition, we experiment with the inclusion of (logical) world knowledge. Our findings suggest that (i) broad performance is sometimes on par, (ii) formula generation is rather brittle, and (iii) world knowledge aids performance relative to data annotation. We argue that these results explicate the weaknesses of both approaches. As such, we consider this study a source of inspiration for future work in the field of neuro-symbolic reasoning.</p>
												</div>
												<div class="workshop-program-abstract" id="talk5">
													<h3><a href="#program" title="Jump up">⤴</a> <strong>Paper:</strong> Building a Compact Math Corpus</h3>
													<p><strong>Andrea Ferreira</strong></p>
													<p>This paper introduces the Compact Math Corpus (CMC), a preliminary resource for natural language processing in the mathematics domain. We process three open-access undergraduate textbooks from distinct mathematical areas and annotate them in the CoNLL-U format using a lightweight pipeline based on the spaCy Small model. The structured output enables the extraction of syntactic bigrams and TF-IDF scores, supporting a syntactic-semantic analysis of mathematical sentences.From the annotated data, we construct a classification dataset comprising bigrams potentially representing mathematical concepts, along with representative example sentences. We combine CMC with the conversational corpus UD English EWT and train a logistic regression model with K-fold cross-validation, achieving a minimum macro-F1 score of 0.989. These results indicate the feasibility of automatic concept identification in mathematical texts. The study is designed for easy replication in low-resource settings and to promote sustainable research practices. Our approach offers a viable path to tasks such as parser adaptation, terminology extraction, multiword expression modeling, and improved analysis of mathematical language structures.</p>
												</div>
												<div class="workshop-program-abstract" id="talk6">
													<h3><a href="#program" title="Jump up">⤴</a> <strong>Abstract:</strong> MERGE: Minimal Expression-Replacement GEneralization Test for Natural Language Inference</h3>
													<p><strong>Mădălina Zgreabăn, Tejaswini Deoskar, Lasha Abzianidze</strong></p>
													<p>In recent years, many NLI benchmarks have shown language models struggle to generalize well. However, these benchmarks are costly to build without automation and can result in unfair comparisons if they are not similar to the models’ training data.  To bridge these gaps, we propose the Minimal Expression-Replacement GEneralization (MERGE) test where we show how to create multiple variants of NLI problems without changing their sentence length, word overlap size, or entailment label. We evaluate NLI models with these variants, where a correct prediction for an NLI problem is when all its variants are classified correctly. Our results suggest that models do not generalize well, as they fail to correctly predict variants of almost 10% of NLI problems, despite them having the same underlying reasoning.</p>
												</div>
												<div class="workshop-program-abstract" id="talk7">
													<h3><a href="#program" title="Jump up">⤴</a> <strong>Contributed talk:</strong> Automatic Evaluation of Linguistic Validity in Japanese CCG Treebanks</h3>
													<p><strong>Asa Tomita, Hitomi Yanaka, Daisuke Bekki</strong></p>
													<p>In Natural Language Inference, the accuracy of systems based on compositional semantics depends on the quality of syntactic analysis, which in turn relies on linguistically valid training and evaluation data, typically provided by treebanks. However, conventional treebank evaluation metrics focus on data coverage and fail to assess the linguistic validity of syntactic structures. This paper proposes novel evaluation methods to enable automatic and multi-faceted assessment of linguistic validity. We apply these methods to a Japanese treebank based on Combinatory Categorial Grammar and report the evaluation results.</p>
												</div>
												<div class="workshop-program-abstract" id="talk8">
													<h3><a href="#program" title="Jump up">⤴</a> <strong>Contributed talk:</strong> How Often Does Natural Logic Actually Meet Machine Learning?</h3>
													<p><strong>Lasha Abzianidze</strong></p>
													<p>Reflecting on the workshop title raises the question of how frequently and to what extent natural logic has been coupled with machine learning in recent years. Unlike other formal logics, natural logic can be seen as more language-friendly, with formulas typically closely following the structure of natural language expressions, and offers certain quick reasoning with natural language. But does this generally make natural logic LLM-friendly, i.e., easier to be successfully paired with LLMs? In this talk, we will survey several works that combined a version of natural logic with deep learning with varying degrees of success. </p>
												</div>
											</div>
										</div>
									</section>

								

								
								<!-- Call for papers -->
									<section id="call-for-papers" class="box features">
										<h2 class="major"><span>Call for Papers</span></h2>
										<div>
											<div class="row">
												<div>
													The NALOMA workshop invites submissions on any (theoretical or computational) aspect of hybrid methods concerning Natural Language Understanding and Reasoning (NLU&R). The topics include but are not limited to:
												</div>
												<ul style="padding-left:3em; margin: 0px; display:block; width: 100vw;">
													<li>Hybrid NLU&R systems that integrate logic-based/symbolic methods with neural networks</li>
													<li>Explainable NLU&R (with structured explanations)</li>
													<li>Opening the black-box of deep learning in NLU&R</li>
													<li>Downstream applications of hybrid NLU&R systems</li>
													<li>Probabilistic semantics for NLU&R</li>
													<li>Comparison and contrast between symbolic and deep learning work on NLU&R</li>
													<li>Creation, criticism, refinement, and augmentation of NLU&R datasets</li>
													<li>(Dis)Alignment of humans and machines on NLU&R tasks</li>
													<li>Addressing inherent human disagreements in NLU&R tasks</li>
													<li>Generalization of NLU&R systems</li>
													<li>Fine-grained evaluation of NLU&R systems</li>
												</ul>
												<div>We invite two types of submissions:</div>

												<ul style="padding-left:3em; margin: 0px; display:block">
													<li><b>Archival (long or short) papers</b> should report on complete, original and unpublished research. Accepted papers will be published in the workshop proceedings and appear in the <a href="https://aclanthology.org/venues/naloma/">ACL anthology</a>.
													Short and long papers may consist of up to 4 and 8 pages of content, respectively, plus unlimited references. Camera-ready versions of papers will be given one additional page of content so that reviewers' comments can be taken into account.
													</li>

													<li><b>Extended abstracts</b> may report on work in progress or work that was recently published/accepted at a different venue. Extended abstracts will not be included in the workshop proceedings. Thus, the unpublished work will retain its status and can be submitted to another venue. This webpage will link to the accepted extended abstracts.
													The extended abstracts should not contain an abstract section and may consist of up to 2 pages of content, plus unlimited references. 
													</li>
												</ul>

												<div>Both accepted papers and extended abstracts are expected to be presented at the workshop. Extended abstracts will be presented as talks or posters at the discretion of the program committee.</div>

												<div>Submissions will be double-blind reviewed, and all long/short papers and extended abstracts must be anonymous, i.e. not reveal author(s) on the title page or through self-references. Both extended abstracts and papers must be formatted according to the <a href="https://github.com/acl-org/acl-style-files">ACL style-files</a> or the ACL <a href="https://www.overleaf.com/latex/templates/association-for-computational-linguistics-acl-conference/jvxskxpnznfj">Overleaf template</a>.  All submissions must adhere to the ARR guidelines about <a href="https://aclrollingreview.org/cfp#instructions-for-two-way-anonymized-review">Anonymized Review</a>, <a href="https://aclrollingreview.org/cfp#authorship">Authorship</a>, <a href="https://aclrollingreview.org/cfp#citation-and-comparison">Citation and Comparison</a>, and <a href="https://aclrollingreview.org/cfp#ethics-policy">Ethics Policy</a> (without requiring completion of the responsible NLP research checklist). 
												</div>	

												<div>Both papers and extended abstracts should be submitted via <a class="moving-glow" href="https://openreview.net/group?id=ESSLLI.eu/2025/Workshop/NALOMA">openreview</a>.</div>

												<div>The workshop participants must register for <a href="https://2025.esslli.eu/">ESSLLI 2025</a>.</div>

											</div>
										</div>
									</section>

								<!-- Important dates -->
									<section id="important-dates" class="box features">
										<h2 class="major"><span>Important Dates</span></h2>
											<div class="row">
												<ul class="listing">
													<!-- <li style="color:rgb(255, 0, 187)">Submission-related dates are tentative</li> -->
													<li>Deadline for papers &amp; extended abstracts: <s>25 April</s> <b>6 May</b></li>
													<li>Notification: <b>27 May</b></li>
													<li>ESSLLI registration dates: <b>31 May</b> (early)</li>
													<li>Camera-ready due: <b>20 June</b></li>
													<li>Workshop: <b>4-8 August</b></li>
													<li style="color:rgb(255, 0, 187)">All dates are AoE</li>
												</ul>
											</div>
									</section>
								<!-- Keynotes -->
								<section id="keynotes" class="box features">
									<h2 class="major"><span>Keynotes</span></h2>
										<div class="row">
											<!-- <span class="keynote-tba">To be announced</span> -->

											<div id="keynote-list" class="keynote-container">
												<div class="keynote">
													<img src="images/aaron.jpg" alt="Aaron Steven White">
													<div class="keynote-info">
														<div class="keynote-name">Aaron Steven White</div>
														<div class="keynote-affiliation">University of Rochester</div>
														<!-- <div>Aaron Steven White is an Associate Professor of Linguistics at the University of Rochester, with a secondary appointment in Computer Science and an affiliation with the Goergen Institute for Data Science. He directs both the Center for Language Sciences and the FACTS.lab (Formal and Computational Semantics Lab) at the University of Rochester. His research focuses on the development of large-scale, theoretically informed semantic annotation frameworks and natural language understanding systems.</div> -->
													</div>
												</div>
										
												<div class="keynote">
													<img src="images/kyle.jpg" alt="Kyle Richardson">
													<div class="keynote-info">
														<div class="keynote-name">Kyle Richardson</div>
														<div class="keynote-affiliation">Allen Institute for AI</div>
														<!-- <div>Kyle Richardson is a senior research scientist at the Allen Institute for AI (AI2) in Seattle. He works at the intersection of NLP and Machine Learning on the Aristo team, with a particular focus on generative AI and language models. Recently, he has been interested in using formal methods to better understand and specify algorithms for large language models. Prior to AI2 he was at the IMS and the University of Stuttgart, where he obtained his PhD in 2018. website: <a href="https://www.krichardson.me"></a>krichardson.me</a></div> -->
													</div>
												</div>

												<div class="keynote">
													<img src="images/mehrnoosh.png" alt="Mehrnoosh Sadrzadeh">
													<div class="keynote-info">
														<div class="keynote-name">Mehrnoosh Sadrzadeh</div>
														<div class="keynote-affiliation">University College London</div>
													</div>
												</div>

												<!-- <div class="keynote">
													<img style="padding: 25px;" src="images/load1.gif" alt="">
													<div class="keynote-info">
														<div class="keynote-name"><span class="cursor"></span></div>
														<div class="keynote-affiliation"><span class="cursor"></span></div>
													</div>
												</div> -->
											</div>
										</div>
								</section>
								<!-- Program committe -->
									<section id="program-committee" class="box features">
										<h2 class="major"><span>Program Committee</span></h2>
										<div>
											<div class="row">
												<ul class="listing" id="PC">
													<li class="cochair"><b>Lasha Abzianidze</b> (co-chair), Utrecht University</li>
													<li class="cochair"><b>Valeria de Paiva</b> (co-chair), Topos Institute</li>
													<li><b>Stergios Chatzikyriakidis</b>, University of Crete</li>
													<li><b>Aikaterini-Lida Kalouli</b>, Bundesdruckerei GmbH</li>
													<li><b>Katrin Erk</b>, University of Texas at Austin</li>
													<li><b>Hai Hu</b>, Shanghai Jiao Tong University</li>
													<li><b>Thomas Icard</b>, Stanford University</li>
													<li><b>Lawrence S. Moss</b>, Indiana University</li>
													<li><b>Hitomi Yanaka</b>, University of Tokyo and Riken Institute</li>
												</ul>
											</div>
										</div>
									</section>

							</div>
						</div>
					</div>
				</section>

				<ul style="display:none" id="keywords">
					<li>Natural Logic Meets Machine Learning</li>
					<li>Formal semantics</li>
					<li>Natural Logic</li>
					<li>Natural Language Processing</li>
					<li>Natural Language Inference</li>
					<li>Computational Linguistics</li>
					<li>Deep Learning</li>
					<li>Linguistics</li>
					<li>Meaning Representation</li>
					<li>Natural Language Reasoning</li>
					<li>Recognizing Textual Entailment</li>
					<li>Semantics</li>
					<li>Meaning</li>
				</ul>

			<!-- Footer -->
				<footer id="footer">
					<div class="container">
						<div class="row gtr-200">
							<!-- <div class="col-12"> -->

								<!-- About -->
									<!-- <section>
										<h2 class="major"><span>What's this about?</span></h2>
										<p>
											This is <strong>TXT</strong>, yet another free responsive site template designed by
											<a href="http://twitter.com/ajlkn">AJ</a> for <a href="http://html5up.net">HTML5 UP</a>. It's released under the
											<a href="http://html5up.net/license/">Creative Commons Attribution</a> license so feel free to use it for
											whatever you're working on (personal or commercial), just be sure to give us credit for the design.
											That's basically it :)
										</p>
									</section> -->

							<!-- </div> -->
							<div class="col-12">

								<!-- Contact -->
									<section>
										<!-- <h2 class="major"><span>Get in touch</span></h2> -->
										<ul class="contact">
											<li><a class="fa-brands fa-x-twitter" style="text-decoration: none;" href="https://x.com/nalomameeting"></a></li>
											<li><a class="fa fa-at" style="text-decoration: none;" href="mailto:naloma.pc@gmail.com"></a></li>
											<li id="copyright" style="float:right; margin-right: auto;">Design: <a style="width:auto; padding: 2px; display:inline; background: none;" href="http://html5up.net">HTML5 UP</a></li>
										</ul>
									</section>

							</div>
						</div>

						<!-- Copyright -->
							<!-- <div id="copyright">
								<ul class="menu">
									<li>&copy; Untitled. All rights reserved</li>
									<li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
								</ul>
							</div> -->
					</div>
				</footer>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="https://cdnjs.cloudflare.com/ajax/libs/fullcalendar/6.1.8/index.global.min.js"></script>
			<script src="assets/js/program.js"></script>
			<script src="assets/js/main.js"></script>

			<script>
				// Shuffle the list when the page loads
				// window.onload = shufflePC;
				// window.onload = shuffleKeynote;

				document.addEventListener("DOMContentLoaded", shufflePC());
				document.addEventListener("DOMContentLoaded", shuffleKeynote(3));
			</script>

	</body>
</html>